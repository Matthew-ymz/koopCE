# EEG æ•°æ®å¤„ç†ä»£ç æ”¹è¿›è¯´æ˜

## ğŸ“‹ é—®é¢˜åˆ†æ

### åŸå§‹ä»£ç ï¼ˆread_data.ipynbï¼‰å­˜åœ¨çš„é—®é¢˜

#### 1. ğŸ› **å…³é”® Bugï¼šæ ‡å‡†åŒ–é€»è¾‘é”™è¯¯**

**åŸå§‹ä»£ç ï¼š**
```python
for i in range(len(all_data)): 
    norm_data = all_data[i].T   # [1700,141]
    scaler = StandardScaler()   # âŒ æ¯æ¬¡éƒ½æ–°å»ºï¼
    scaler.fit(norm_data)       # âŒ ç”¨å½“å‰æ ·æœ¬çš„ç»Ÿè®¡é‡
    norm_data = scaler.transform(norm_data) 
    all_norm_data.append(norm_data)
```

**é—®é¢˜åˆ†æï¼š**
- æ¯ä¸ªæ ·æœ¬å•ç‹¬åˆ›å»º scalerï¼Œå¹¶ç”¨è¯¥æ ·æœ¬çš„å‡å€¼å’Œæ–¹å·®è¿›è¡Œæ ‡å‡†åŒ–
- å¯¼è‡´æ¯ä¸ªæ ·æœ¬çš„æ ‡å‡†åŒ–å‚æ•°ä¸åŒï¼Œ**ç ´åäº†æ•°æ®åˆ†å¸ƒçš„ä¸€è‡´æ€§**
- è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ— æ³•ç”¨ç»Ÿä¸€çš„æ ‡å‡†åŒ–å‚æ•°è½¬æ¢
- æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šå­¦åˆ°çš„æ ‡å‡†åŒ–è§„å¾‹æ— æ³•åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨

**åæœï¼š**
- æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸¥é‡ä¸‹é™
- ç»“æœä¸å¯å¤ç°
- ç»Ÿè®¡å­¦æ„ä¹‰ä¸æ˜ç¡®

---

#### 2. ğŸ“‹ **ä»£ç é‡å¤é—®é¢˜**

**åŸå§‹ä»£ç ä¸­è®­ç»ƒé›†å’Œæµ‹è¯•é›†å¤„ç†é€»è¾‘å®Œå…¨ç›¸åŒï¼š**

```python
# ---- è®­ç»ƒé›†å¤„ç† ----
for j in range(0, 3):
    train_input, train_target = [], []
    for i in range(train_ratio):
        if j == 0: 
            stage_data = all_norm_data[i][:600] 
        elif j == 1: 
            stage_data = all_norm_data[i][500:1100]
        elif j == 2:
            stage_data = all_norm_data[i][1100:1700]
        train_input.append(stage_data[:-1]) 
        train_target.append(stage_data[1:]) 
    # ... ä¿å­˜é€»è¾‘

# ---- æµ‹è¯•é›†å¤„ç† ----
for j in range(0, 3):
    test_input, test_target = [], []
    for i in range(train_ratio, len(all_norm_data)):  # â† ä»…æ­¤ä¸åŒ
        if j == 0:
            stage_data = all_norm_data[i][:600]
        # ... å®Œå…¨ç›¸åŒçš„ä»£ç 
```

**é—®é¢˜ï¼š**
- DRY åŸåˆ™è¿å (Don't Repeat Yourself)
- ä¿®æ”¹ä¸€å¤„éœ€è¦æ”¹ä¸¤å¤„
- å®¹æ˜“å¼•å…¥ä¸ä¸€è‡´çš„ bug
- ä»£ç å¯ç»´æŠ¤æ€§å·®

---

#### 3. ğŸ”¢ **é­”æ•°ç¡¬ç¼–ç **

**åˆ†æ•£åœ¨ä»£ç ä¸­çš„ç¡¬ç¼–ç å€¼ï¼š**

| ç¡¬ç¼–ç å€¼ | å«ä¹‰ | ä½ç½® |
|---------|------|------|
| `[:12]` | å–å‰ 12 ä¸ªæ ·æœ¬ | æ•°æ®åŠ è½½å¤„ |
| `1, 1` | æ³¨æ„åŠ›ç±»å‹å’Œè¯•éªŒç±»å‹ | ç­›é€‰æ¡ä»¶ |
| `1700` | æ€»æ—¶é—´çª—å£ | å¤šå¤„å‡ºç° |
| `[:600]`, `[500:1100]`, `[1100:1700]` | é˜¶æ®µåˆ†å‰²ç´¢å¼• | å¾ªç¯å†… 6 å¤„ |

**é—®é¢˜ï¼š**
- æ— æ³•è½»æ˜“ä¿®æ”¹å‚æ•°è¿›è¡Œä¸åŒå®éªŒ
- ä»£ç çš„é€šç”¨æ€§å·®
- éš¾ä»¥ç»´æŠ¤å’Œæ‰©å±•

---

#### 4. ğŸ” **å˜é‡ä½œç”¨åŸŸå’Œå‘½åé—®é¢˜**

```python
train_input.shape  # âŒ æœ€åå¾ªç¯çš„ train_input æ˜¯ä»€ä¹ˆï¼Ÿ
test_input.shape   # âŒ æœ€åå¾ªç¯çš„ test_input æ˜¯ä»€ä¹ˆï¼Ÿ
```

**é—®é¢˜ï¼š**
- åœ¨å¾ªç¯ä¸­å¤šæ¬¡å®šä¹‰ç›¸åŒå˜é‡å
- æœ€åè¾“å‡ºçš„æ˜¯ç¬¬ 3 é˜¶æ®µï¼ˆj=2ï¼‰çš„æ•°æ®
- å®¹æ˜“æ··æ·†å’Œè¯¯è§£

---

#### 5. âŒ **ç¼ºå°‘é”™è¯¯å¤„ç†**

```python
with h5py.File('./IPCAS_ExemplarData_ZXL_Sub14.mat', 'r') as f: 
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ä¼šå´©æºƒ
    # å¦‚æœæ•°æ®ç»“æ„ä¸åŒä¼šæŠ¥é”™
    # æ²¡æœ‰ä»»ä½•æç¤º
```

---

#### 6. ğŸ“‚ **ç¼ºå°‘ç›®å½•åˆ›å»ºé€»è¾‘**

```python
pd.DataFrame(train_input).to_csv('./visual_inducted_conscious/1/stage%s/train_input.csv'%(j+1), 
                                 header=None, index=None)
# âŒ å¦‚æœç›®å½•ä¸å­˜åœ¨ä¼šæŠ¥é”™ï¼
```

---

## âœ¨ æ”¹è¿›æ–¹æ¡ˆ

### æ”¹è¿›ç‰ˆæœ¬ï¼š`read_data_improved.py`

#### 1. âœ… **ä¿®å¤æ ‡å‡†åŒ–é€»è¾‘**

```python
def normalize_data(all_data):
    """å¯¹æ‰€æœ‰æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†"""
    all_norm_data = []
    scaler = StandardScaler()  # âœ… åªåˆ›å»ºä¸€æ¬¡
    
    for i, data in enumerate(all_data):
        data_transposed = data.T
        normalized = scaler.fit_transform(data_transposed)  # âœ… åŒä¸€ scaler
        all_norm_data.append(normalized)
    
    return all_norm_data, scaler  # âœ… è¿”å› scaler ç”¨äºæµ‹è¯•é›†
```

**æ”¹è¿›ï¼š**
- ä½¿ç”¨å…¨å±€ scaler ç¡®ä¿ä¸€è‡´æ€§
- å¯ä»¥ä¿å­˜å’ŒåŠ è½½ scaler ç”¨äºæ–°æ•°æ®
- ç¬¦åˆæ ‡å‡†çš„æ•°æ®é¢„å¤„ç†æµç¨‹

---

#### 2. âœ… **å‚æ•°é…ç½®é›†ä¸­ç®¡ç†**

```python
class Config:
    """é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    
    # æ–‡ä»¶è·¯å¾„
    INPUT_FILE = './IPCAS_ExemplarData_ZXL_Sub14.mat'
    OUTPUT_BASE_DIR = './visual_inducted_conscious/1'
    
    # æ•°æ®è¿‡æ»¤æ¡ä»¶
    ATTEN_TYPE_FILTER = 1      # æ³¨æ„åŠ›ç±»å‹
    TRIAL_TYPE_FILTER = 1      # è¯•éªŒç±»å‹
    
    # æ•°æ®å¤„ç†å‚æ•°
    TOTAL_INTERVAL = 1700
    TEST_RATIO = 0.04
    
    # é˜¶æ®µåˆ†å‰²å®šä¹‰ï¼ˆé›†ä¸­ç®¡ç†ï¼‰
    STAGE_DEFINITIONS = {
        1: (0, 600),
        2: (500, 1100),
        3: (1100, 1700),
    }
    
    VERBOSE = True             # è°ƒè¯•æ¨¡å¼
```

**æ”¹è¿›ï¼š**
- æ‰€æœ‰å‚æ•°ä¸€ç›®äº†ç„¶
- ä¿®æ”¹å‚æ•°åªéœ€æ”¹é…ç½®ç±»
- æ˜“äºè¿›è¡Œå‚æ•°æ•æ„Ÿæ€§åˆ†æ
- æ”¯æŒå¤šä¸ªé…ç½®æ–¹æ¡ˆ

---

#### 3. âœ… **æ¶ˆé™¤ä»£ç é‡å¤**

```python
def prepare_sequences(data_list, stage_num):
    """ä¸ºç‰¹å®šé˜¶æ®µå‡†å¤‡è¾“å…¥-ç›®æ ‡åºåˆ—å¯¹"""
    inputs, targets = [], []
    
    for data in data_list:
        stage_data = extract_stage_data(data, stage_num)
        inputs.append(stage_data[:-1])
        targets.append(stage_data[1:])
    
    return np.concatenate(inputs, axis=0), np.concatenate(targets, axis=0)


def process_all_stages(train_data, test_data):
    """å¤„ç†æ‰€æœ‰é˜¶æ®µçš„æ•°æ®å¹¶ä¿å­˜"""
    
    for stage_num in Config.STAGE_DEFINITIONS.keys():
        # å¤„ç†è®­ç»ƒé›†
        train_input, train_target = prepare_sequences(train_data, stage_num)
        save_data(train_input, train_target, stage_num, 'train')
        
        # å¤„ç†æµ‹è¯•é›†
        test_input, test_target = prepare_sequences(test_data, stage_num)
        save_data(test_input, test_target, stage_num, 'test')
```

**æ”¹è¿›ï¼š**
- ç»Ÿä¸€å¤„ç†é€»è¾‘
- æ²¡æœ‰ä»£ç é‡å¤
- æ˜“äºç»´æŠ¤

---

#### 4. âœ… **å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—**

```python
def load_raw_data(filepath: str):
    """ä» MAT æ–‡ä»¶è¯»å–åŸå§‹ EEG æ•°æ®"""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    try:
        with h5py.File(filepath, 'r') as f:
            # ... è¯»å–æ•°æ®
            if Config.VERBOSE:
                print(f"EEG æ•°æ®å½¢çŠ¶: {bv_group.shape}")
                print(f"è¯•éªŒç±»å‹: {np.unique(trial_types)}")
        return data
    except Exception as e:
        raise RuntimeError(f"è¯»å– MAT æ–‡ä»¶å¤±è´¥: {e}")
```

**æ”¹è¿›ï¼š**
- æ–‡ä»¶å­˜åœ¨æ£€æŸ¥
- å¼‚å¸¸å¤„ç†å’Œæç¤º
- è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—

---

#### 5. âœ… **è‡ªåŠ¨åˆ›å»ºç›®å½•**

```python
def save_data(input_data, target_data, stage_num, split_type='train'):
    """ä¿å­˜æ•°æ®ä¸º CSV æ–‡ä»¶"""
    # åˆ›å»ºç›®å½•
    stage_dir = os.path.join(Config.OUTPUT_BASE_DIR, f'stage{stage_num}')
    os.makedirs(stage_dir, exist_ok=True)  # âœ… è‡ªåŠ¨åˆ›å»º
    
    # ä¿å­˜æ•°æ®
    input_path = os.path.join(stage_dir, f'{split_type}_input.csv')
    pd.DataFrame(input_data).to_csv(input_path, header=False, index=False)
```

---

#### 6. âœ… **ç±»å‹æç¤ºå’Œæ–‡æ¡£**

```python
def filter_and_extract_data(
    bv_group: np.ndarray,           # â† ç±»å‹æç¤º
    trial_types: np.ndarray,
    atten_types: np.ndarray,
    time1: np.ndarray,
    atten_type: int,
    trial_type: int,
    interval: int
) -> List[np.ndarray]:             # â† è¿”å›ç±»å‹
    """
    æŒ‰æ¡ä»¶ç­›é€‰æ•°æ®å¹¶æå–å›ºå®šé•¿åº¦çš„æ•°æ®æ®µ
    
    Args:
        bv_group: åŸå§‹ EEG æ•°æ®
        ...
        
    Returns:
        æå–çš„æ•°æ®æ®µåˆ—è¡¨
    """
```

---

## ğŸ“Š æ”¹è¿›å¯¹æ¯”è¡¨

| ç‰¹æ€§ | åŸå§‹ä»£ç  | æ”¹è¿›å |
|------|---------|--------|
| **æ ‡å‡†åŒ–é€»è¾‘** | âŒ æ¯æ ·æœ¬ç‹¬ç«‹ | âœ… å…¨å±€ç»Ÿä¸€ |
| **ä»£ç é‡å¤** | âŒ 6 å¤„é‡å¤ | âœ… DRY åŸåˆ™ |
| **å‚æ•°ç®¡ç†** | âŒ åˆ†æ•£ç¡¬ç¼–ç  | âœ… é›†ä¸­é…ç½® |
| **é”™è¯¯å¤„ç†** | âŒ æ—  | âœ… å®Œæ•´ |
| **ç›®å½•ç®¡ç†** | âŒ æ‰‹åŠ¨åˆ›å»º | âœ… è‡ªåŠ¨åˆ›å»º |
| **ä»£ç æ–‡æ¡£** | âŒ æ—  | âœ… è¯¦ç»†æ³¨é‡Š |
| **ç±»å‹æç¤º** | âŒ æ—  | âœ… å®Œæ•´ |
| **è°ƒè¯•ä¿¡æ¯** | âŒ æ—  | âœ… è¯¦ç»†æ—¥å¿— |
| **å¯ç»´æŠ¤æ€§** | âŒ ä½ | âœ… é«˜ |
| **å¯æ‰©å±•æ€§** | âŒ ä½ | âœ… é«˜ |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³• 1: åœ¨ Jupyter Notebook ä¸­ä½¿ç”¨

```python
# åœ¨ notebook ä¸­å¯¼å…¥
import sys
sys.path.append('./koopCE/neural_science')
from read_data_improved import main, Config

# è¿è¡Œä¸»ç¨‹åº
main()

# æˆ–ä¿®æ”¹é…ç½®åè¿è¡Œ
Config.TEST_RATIO = 0.1
Config.VERBOSE = False
main()
```

### æ–¹æ³• 2: å‘½ä»¤è¡Œè¿è¡Œ

```bash
cd koopCE/neural_science
python read_data_improved.py
```

### æ–¹æ³• 3: é€æ­¥è°ƒè¯•

```python
from read_data_improved import *

# Step 1: åŠ è½½æ•°æ®
bv_group, trial_types, atten_types, time1 = load_raw_data(Config.INPUT_FILE)

# Step 2: ç­›é€‰å’Œæå–
all_data = filter_and_extract_data(
    bv_group, trial_types, atten_types, time1,
    Config.ATTEN_TYPE_FILTER, Config.TRIAL_TYPE_FILTER, Config.TOTAL_INTERVAL
)
print(f"æå–çš„æ ·æœ¬æ•°: {len(all_data)}")

# Step 3: æ ‡å‡†åŒ–
all_norm_data, scaler = normalize_data(all_data)

# Step 4: åˆ’åˆ†
train_ratio, train_data, test_data = train_test_split(all_norm_data)

# Step 5: å¤„ç†å’Œä¿å­˜
process_all_stages(train_data, test_data)
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒæ”¹è¿›

1. **ä¿®å¤æ ‡å‡†åŒ– Bug** - ä½¿ç”¨å…¨å±€ scaler ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
2. **æ¶ˆé™¤ä»£ç é‡å¤** - éµå¾ª DRY åŸåˆ™ï¼Œæå–å…¬å…±é€»è¾‘
3. **å‚æ•°é›†ä¸­ç®¡ç†** - Config ç±»ç®¡ç†æ‰€æœ‰å‚æ•°
4. **å®Œæ•´æ–‡æ¡£** - è¯¦ç»†çš„æ–‡æ¡£å­—ç¬¦ä¸²å’Œç±»å‹æç¤º
5. **å¼ºå¤§çš„é”™è¯¯å¤„ç†** - è‡ªåŠ¨åˆ›å»ºç›®å½•ã€æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
6. **è°ƒè¯•å‹å¥½** - è¯¦ç»†çš„æ—¥å¿—è¾“å‡ºæ”¯æŒé€æ­¥è°ƒè¯•

### å‘åå…¼å®¹æ€§

æ”¹è¿›åçš„ä»£ç äº§ç”Ÿä¸åŸå§‹ä»£ç ç›¸åŒçš„è¾“å‡ºæ–‡ä»¶ç»“æ„ï¼ˆå‡è®¾ Bug è¢«ä¿®å¤ï¼‰ã€‚

### æ‰©å±•æ€§

- æ˜“äºæ·»åŠ æ–°çš„é˜¶æ®µå®šä¹‰
- æ˜“äºæ”¹å˜è¿‡æ»¤æ¡ä»¶
- æ˜“äºå¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼

---

**æ–‡ä»¶ä½ç½®**: `koopCE/neural_science/read_data_improved.py`

**å»ºè®®**: ä½¿ç”¨æ”¹è¿›ç‰ˆæœ¬æ›¿æ¢åŸå§‹ notebookï¼Œæˆ–åœ¨ä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´è¿›è¡ŒéªŒè¯å¯¹æ¯”ã€‚
